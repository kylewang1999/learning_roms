# Overview
The PPO training function has many parameters that can be adjusted to optimize training performance. Below is a description of the key parameters straight from ```ppo.train()``` function.
`environment`: the environment to train

`num_timesteps`: the total number of environment steps to use during training

`max_devices_per_host`: maximum number of chips to use per host process

`wrap_env`: If `True`, wrap the environment for training. Otherwise use the environment as is.

`madrona_backend`: whether to use Madrona backend for training

`augment_pixels`: whether to add image augmentation to pixel inputs

`num_envs`: the number of parallel environments to use for rollouts  
    NOTE: `num_envs` must be divisible by the total number of chips since each
    chip gets `num_envs // total_number_of_chips` environments to roll out  
    NOTE: `batch_size * num_minibatches` must be divisible by `num_envs` since
    data generated by `num_envs` parallel envs gets used for gradient
    updates over `num_minibatches` of data, where each minibatch has a
    leading dimension of `batch_size`

`episode_length`: the length of an environment episode

`action_repeat`: the number of timesteps to repeat an action

`wrap_env_fn`: a custom function that wraps the environment for training. If not specified, the environment is wrapped with the default training wrapper.

`randomization_fn`: a user-defined callback function that generates randomized environments

`learning_rate`: learning rate for PPO loss

`entropy_cost`: entropy reward for PPO loss, higher values increase entropy of the policy

`discounting`: discounting rate

`unroll_length`: the number of timesteps to unroll in each environment. The PPO loss is computed over 
`unroll_length` timesteps

`batch_size`: the batch size for each minibatch SGD step

`num_minibatches`: the number of times to run the SGD step, each with adifferent minibatch with leading dimension of `batch_size`

`num_updates_per_batch`: the number of times to run the gradient update over all minibatches before 
doing a new environment rollout

`num_resets_per_eval`: the number of environment resets to run between each eval. The environment resets occur on the host

`normalize_observations`: whether to normalize observations

`reward_scaling`: float scaling for reward

`clipping_epsilon`: clipping epsilon for PPO loss

`gae_lambda`: General advantage estimation lambda

`max_grad_norm`: gradient clipping norm value. If `None`, no clipping is done

`normalize_advantage`: whether to normalize advantage estimate

`network_factory`: function that generates networks for policy and value functions

`seed`: random seed

`num_evals`: the number of evals to run during the entire training run. Increasing the number of evals increases total training time

`eval_env`: an optional environment for eval only, defaults to `environment`

`num_eval_envs`: the number of envs to use for evaluation. Each env will run 1 episode, and all envs run in parallel during eval.

`deterministic_eval`: whether to run the eval with a deterministic policy

`log_training_metrics`: whether to log training metrics and callback to `progress_fn`

`training_metrics_steps`: the number of environment steps between logging training metrics

`progress_fn`: a user-defined callback function for reporting/plotting metrics

`policy_params_fn`: a user-defined callback function that can be used for saving custom policy checkpoints or creating policy rollouts and videos

`save_checkpoint_path`: the path used to save checkpoints. If `None`, no checkpoints are saved.

`restore_checkpoint_path`: the path used to restore previous model params

`restore_params`: raw network parameters to restore the `TrainingState` from. These override 
`restore_checkpoint_path`. These parameters can be obtained from the return values of `ppo.train()`.

`restore_value_fn`: whether to restore the value function from the checkpoint or use a random initialization

`run_evals`: if `True`, use the evaluator `num_eval` times to collect distinct eval rollouts. If `False`, `num_eval_envs` and `eval_env` are ignored.  `progress_fn` is then expected to use training metrics.
