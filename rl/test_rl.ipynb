{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac341e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brax imports\n",
    "import functools\n",
    "import jax\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from jax import numpy as jp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import brax\n",
    "import flax\n",
    "from brax import envs\n",
    "from brax.io import model\n",
    "from brax.io import json\n",
    "from brax.io import html\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.sac import train as sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f42e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom imports\n",
    "from envs.cart_pole_env import CartPoleEnv, CartPoleConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the environment\n",
    "env = CartPoleEnv(CartPoleConfig()) \n",
    "\n",
    "# select some hyperparameters (for ppo.train)\n",
    "num_timesteps = 1_000_000  # total number of training timesteps\n",
    "num_evals = 20             # number of times to evaluate the policy\n",
    "reward_scaling = 10.0      # scaling factor on the reward\n",
    "episode_length = 1000     # max length of each episode\n",
    "normalize_observations = True # whether to normalize observations\n",
    "action_repeat = 1         # number of env steps per action\n",
    "unroll_length = 32        # number of steps to unroll the policy\n",
    "num_minibatches = 32      # number of minibatches for ppo\n",
    "num_updates_per_batch = 8 # number of updates per batch\n",
    "discounting = 0.97        # discounting factor for future rewards\n",
    "learning_rate = 3e-4      # learning rate for the optimizer\n",
    "entropy_cost = 1e-2       # cost for the policy entropy\n",
    "num_envs = 2048           # number of parallel environments\n",
    "batch_size = 1024         # number of samples per gradient update\n",
    "seed = 0                 # random seed for reproducibility\n",
    "\n",
    "# make the train function\n",
    "train_fn = functools.partial(\n",
    "    ppo.train,\n",
    "    env=env,\n",
    "    num_timesteps=num_timesteps,\n",
    "    num_evals=num_evals,\n",
    "    reward_scaling=reward_scaling,\n",
    "    episode_length=episode_length,\n",
    "    normalize_observations=normalize_observations,\n",
    "    action_repeat=action_repeat,\n",
    "    unroll_length=unroll_length,\n",
    "    num_minibatches=num_minibatches,\n",
    "    num_updates_per_batch=num_updates_per_batch,\n",
    "    discounting=discounting,\n",
    "    learning_rate=learning_rate,\n",
    "    entropy_cost=entropy_cost,\n",
    "    num_envs=num_envs,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
