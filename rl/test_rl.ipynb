{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac341e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brax imports\n",
    "import functools\n",
    "import jax\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from jax import numpy as jp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import brax\n",
    "import flax\n",
    "from brax import envs\n",
    "from brax.io import model\n",
    "from brax.io import json\n",
    "from brax.io import html\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.sac import train as sac\n",
    "\n",
    "# custom imports\n",
    "from envs.cart_pole_env import CartPoleEnv, CartPoleConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the device being used (gpu or cpu)\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the environment\n",
    "env = CartPoleEnv(CartPoleConfig()) \n",
    "\n",
    "# select some hyperparameters (for ppo.train)\n",
    "num_timesteps = 10_00_000  # total number of training timesteps\n",
    "num_evals = 20             # number of times to evaluate the policy\n",
    "reward_scaling = 10.0      # scaling factor on the reward\n",
    "episode_length = 750     # max length of each episode\n",
    "normalize_observations = True # whether to normalize observations\n",
    "action_repeat = 1         # number of env steps per action\n",
    "unroll_length = 8        # number of steps to unroll the policy\n",
    "num_minibatches = 32      # number of minibatches for ppo\n",
    "num_updates_per_batch = 8 # number of updates per batch\n",
    "discounting = 0.97        # discounting factor for future rewards\n",
    "learning_rate = 3e-4      # learning rate for the optimizer\n",
    "entropy_cost = 1e-2       # cost for the policy entropy\n",
    "num_envs = 2048           # number of parallel environments\n",
    "batch_size = 1024         # number of samples per gradient update\n",
    "seed = 1                 # random seed for reproducibility\n",
    "\n",
    "# make the train function\n",
    "train_fn = functools.partial(\n",
    "    ppo.train,\n",
    "    environment=env,\n",
    "    num_timesteps=num_timesteps,\n",
    "    num_evals=num_evals,\n",
    "    reward_scaling=reward_scaling,\n",
    "    episode_length=episode_length,\n",
    "    normalize_observations=normalize_observations,\n",
    "    action_repeat=action_repeat,\n",
    "    unroll_length=unroll_length,\n",
    "    num_minibatches=num_minibatches,\n",
    "    num_updates_per_batch=num_updates_per_batch,\n",
    "    discounting=discounting,\n",
    "    learning_rate=learning_rate,\n",
    "    entropy_cost=entropy_cost,\n",
    "    num_envs=num_envs,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "max_y = 8000\n",
    "min_y = -100\n",
    "\n",
    "# containers for the data\n",
    "xdata, ydata = [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  times.append(datetime.now())\n",
    "  xdata.append(num_steps)\n",
    "  \n",
    "  # select a reward key that exists\n",
    "  if 'reward_total' in metrics:\n",
    "      ydata.append(metrics['reward_total'])\n",
    "      print(\"found rewrad_total\")\n",
    "  elif 'eval/episode_reward' in metrics:  # fallback key for eval metrics\n",
    "      ydata.append(metrics['eval/episode_reward'])\n",
    "      print(\"found eval/episode_reward\")\n",
    "  else:\n",
    "      # default if no reward info yet\n",
    "      ydata.append(0.0)\n",
    "      print(\"no reward found\")\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.xlim([0, train_fn.keywords['num_timesteps']])\n",
    "  plt.ylim([min_y, max_y])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.plot(xdata, ydata)\n",
    "  plt.show()\n",
    "\n",
    "# train\n",
    "make_inference_fn, params, metrics = train_fn(environment=env, progress_fn=progress)\n",
    "\n",
    "# print timing info\n",
    "print(f'time to jit: {times[1] - times[0]}')\n",
    "print(f'time to train: {times[-1] - times[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f60065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the metrics\n",
    "for key, val in metrics.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
